{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure Generation for the paper\n",
    "\n",
    "## Data Ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('/home/ubuntu/BioDSA') # this will need to be updated for other machines/folders\n",
    "from src import (\n",
    "    REPO_ROOT,\n",
    "    TOP_LEVEL_LOG_DIR\n",
    ")\n",
    "\n",
    "logs_directory_path = os.path.join(TOP_LEVEL_LOG_DIR, \"experiment_logs\")\n",
    "\n",
    "def get_experiment_results(logs_directory_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Get the results of all experiments in the given directory.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for log_file in os.listdir(logs_directory_path):\n",
    "        if not log_file.endswith(\".json\") or \"experiment|\" not in log_file:\n",
    "            continue\n",
    "        with open(os.path.join(logs_directory_path, log_file), \"r\") as f:\n",
    "            try:\n",
    "                results.append(json.load(f))\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error decoding JSON from {log_file}\")\n",
    "    return results\n",
    "\n",
    "results = get_experiment_results(logs_directory_path)\n",
    "\n",
    "# create a dataframe from the results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# unpack the experiment_config column into separate columns\n",
    "df = pd.concat([df.drop('experiment_config', axis=1), df['experiment_config'].apply(pd.Series)], axis=1)\n",
    "\n",
    "# extract the agent type, which is composed of the agent_type, and it's hyperparameters\n",
    "def get_agent_type(row: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Format agent type based on configuration.\n",
    "    For react agent: (react, step_count)\n",
    "    For reasoning coder: (reasoning_coder, planning_model, coding_model)\n",
    "    For coder: (coder, model_name)\n",
    "    \"\"\"\n",
    "    agent_type = row[\"agent_type\"]\n",
    "    if agent_type == \"react\":\n",
    "        return f\"(react, {row['step_count']}, {row['model_name']})\"\n",
    "    elif agent_type == \"reasoning_coder\":\n",
    "        return f\"(reasoning_coder, {row['planning_model']}, {row['coding_model']})\"\n",
    "    elif agent_type == \"coder\":\n",
    "        return f\"(coder, {row['model_name']})\"\n",
    "    elif agent_type == \"reasoning_react\":\n",
    "        return f\"(reasoning_react, {row['plan_model_name']}, {row['agent_model_name']}, {row['step_count']})\"\n",
    "    elif agent_type == \"reasoning_react_v2\":\n",
    "        return f\"(reasoning_react_v2, {row['plan_model_name']}, {row['agent_model_name']}, {row['step_count']})\"\n",
    "    return agent_type\n",
    "\n",
    "df['agent_summary'] = df.apply(get_agent_type, axis=1)\n",
    "df['agent_summary'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Result Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check in case some of the llms outputted weird values\n",
    "display(df['final_answer'].value_counts()) # generated by the agents\n",
    "display(df['hypothesis_is_true'].value_counts()) # ground truth\n",
    "\n",
    "# Standardize the final answer values\n",
    "df['coerced_final_answer'] = df['final_answer'].astype(str).str.lower()\n",
    "df['hypothesis_is_true_coerced'] = df['hypothesis_is_true'].astype(str).str.lower()\n",
    "\n",
    "# Check unique values after standardization\n",
    "print(\"Unique values in coerced_final_answer before standardization:\")\n",
    "display(df['coerced_final_answer'].value_counts())\n",
    "print(\"Unique values in hypothesis_is_true_coerced before standardization:\")\n",
    "display(df['hypothesis_is_true_coerced'].value_counts())\n",
    "\n",
    "def standardize_agent_response(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Standardize the agent response to a consistent format.\n",
    "    \"\"\"\n",
    "    if 'true' in response:\n",
    "        return 'true'\n",
    "    elif 'false' in response:\n",
    "        return 'false'\n",
    "    else:\n",
    "        return 'not verifiable'\n",
    "\n",
    "# Create a cleaner version of final answer that handles variations\n",
    "df['clean_final_answer'] = df['coerced_final_answer'].apply(\n",
    "    standardize_agent_response\n",
    ")\n",
    "\n",
    "print(\"Unique values in clean_final_answer after standardization:\")\n",
    "display(df['clean_final_answer'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executability Rate\n",
    "Furthermore, we evaluate the technical quality of the generated code. For each hypothesis, let $C$\n",
    "denote the total number of code cells generated and $C_{exec}$ the number of those that are executable\n",
    "without error. The code executability rate is defined as\n",
    "$$\n",
    "\\text{Executability Rate} = \\frac{C_{exec}}{C}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple, List\n",
    "\n",
    "def extract_error_metadata(observations: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract error metadata from observations containing stack traces.\n",
    "    \n",
    "    Args:\n",
    "        observations: String containing the execution output\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with error metadata including:\n",
    "        - has_error: Boolean indicating if a traceback was found\n",
    "        - error_type: The type of exception if found (e.g., KeyError, ValueError)\n",
    "        - error_message: The error message if found\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    result = {\n",
    "        \"has_error\": False,\n",
    "        \"error_type\": None,\n",
    "        \"error_message\": None\n",
    "    }\n",
    "    \n",
    "    # Check if there's a traceback in the observations\n",
    "    if 'Traceback' in observations:\n",
    "        result[\"has_error\"] = True\n",
    "        \n",
    "        # Try to extract the error type and message using regex\n",
    "        # This pattern looks for the error type at the end of a traceback\n",
    "        # Updated to handle module-specific errors like _csv.Error\n",
    "        error_pattern = r'Traceback \\(most recent call last\\):.*?([A-Za-z0-9_\\.]+(?:Error|Exception)):\\s*([^\\n]+)'\n",
    "        match = re.search(error_pattern, observations, re.DOTALL)\n",
    "        \n",
    "        if match:\n",
    "            result[\"error_type\"] = match.group(1)\n",
    "            result[\"error_message\"] = match.group(2).strip()\n",
    "        else:\n",
    "            # If we found a traceback but couldn't parse the error type, log a warning\n",
    "            print(f\"WARNING: Traceback found but couldn't parse error type from: {observations[:200]}...\")\n",
    "            \n",
    "            # Try a more lenient pattern to catch other exception types\n",
    "            alt_pattern = r'Traceback \\(most recent call last\\):.*?([A-Za-z0-9_\\.]+):\\s*([^\\n]+)'\n",
    "            alt_match = re.search(alt_pattern, observations, re.DOTALL)\n",
    "            \n",
    "            if alt_match:\n",
    "                result[\"error_type\"] = alt_match.group(1)\n",
    "                result[\"error_message\"] = alt_match.group(2).strip()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def extract_react_code_cells(observations: str) -> List[Tuple[int, str]]:\n",
    "    \"\"\"\n",
    "    Extract code cells and their corresponding stdout from React agent observations.\n",
    "    \n",
    "    Args:\n",
    "        observations: String containing the React agent observations\n",
    "        \n",
    "    Returns:\n",
    "        List of tuples (observation_number, stdout) for each code cell\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Pattern to match observation blocks\n",
    "    observation_pattern = r'## Observation (\\d+)\\n### Code: \\n```.*?\\n(.*?)\\n```\\n### Stdout:\\n(.*?)(?=\\n## Observation|\\Z)'\n",
    "    \n",
    "    # Find all matches\n",
    "    matches = re.findall(observation_pattern, observations, re.DOTALL)\n",
    "    \n",
    "    # Extract observation number and stdout for each match\n",
    "    result = []\n",
    "    for match in matches:\n",
    "        observation_num = int(match[0])\n",
    "        code = match[1]\n",
    "        stdout = match[2]\n",
    "        result.append((observation_num, stdout))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def process_observations(row_dict: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Process the observations column to extract the number of code cells and the number of executable code cells\n",
    "    and extract error metadata if present.\n",
    "    \"\"\"\n",
    "    agent_type = row_dict['agent_type']\n",
    "    observations = row_dict['observations']\n",
    "    total_code_cells = 0\n",
    "    executable_code_cells = 0\n",
    "    \n",
    "    if agent_type == 'coder' or agent_type == 'reasoning_coder':\n",
    "        total_code_cells = 1\n",
    "        \n",
    "        # Extract error metadata\n",
    "        error_data = extract_error_metadata(observations)\n",
    "        \n",
    "        # Add error metadata to the row dictionary\n",
    "        row_dict['has_error'] = error_data['has_error']\n",
    "        row_dict['error_type'] = error_data['error_type']\n",
    "        row_dict['error_message'] = error_data['error_message']\n",
    "        \n",
    "        if not error_data['has_error']:\n",
    "            executable_code_cells = 1\n",
    "        \n",
    "    elif agent_type == 'react' or agent_type == 'reasoning_react' or agent_type == 'reasoning_react_v2':\n",
    "        # Extract code cells and their stdout\n",
    "        code_cells = extract_react_code_cells(observations)\n",
    "        \n",
    "        # Calculate total code cells\n",
    "        total_code_cells = len(code_cells)\n",
    "        \n",
    "        # Check each stdout for errors\n",
    "        error_types = []\n",
    "        error_messages = []\n",
    "        \n",
    "        for _, stdout in code_cells:\n",
    "            error_data = extract_error_metadata(stdout)\n",
    "            if not error_data['has_error']:\n",
    "                executable_code_cells += 1\n",
    "            else:\n",
    "                if error_data['error_type']:\n",
    "                    error_types.append(error_data['error_type'])\n",
    "                if error_data['error_message']:\n",
    "                    error_messages.append(error_data['error_message'])\n",
    "        \n",
    "        # Store error information\n",
    "        row_dict['has_error'] = total_code_cells > executable_code_cells\n",
    "        row_dict['error_type'] = error_types\n",
    "        row_dict['error_message'] = error_messages\n",
    "    \n",
    "    row_dict['total_code_cells'] = total_code_cells\n",
    "    row_dict['executable_code_cells'] = executable_code_cells\n",
    "    \n",
    "    return row_dict\n",
    "\n",
    "# Function to analyze executability rate by agent type\n",
    "def analyze_executability_rate(df):\n",
    "    \"\"\"\n",
    "    Analyze the executability rate by agent type.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the processed observations\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with executability rate by agent type\n",
    "    \"\"\"\n",
    "    print(df.index)\n",
    "    # Process all observations\n",
    "    processed_df = df.apply(process_observations, axis=1)\n",
    "    # check that the index is not duplicated\n",
    "    assert not processed_df.index.duplicated().any(), \"Index is duplicated\"\n",
    "    \n",
    "    # Group by agent type and calculate executability rate\n",
    "    executability = processed_df.groupby('agent_type').apply(\n",
    "        lambda x: {\n",
    "            'total_code_cells': x['total_code_cells'].sum(),\n",
    "            'executable_code_cells': x['executable_code_cells'].sum(),\n",
    "            'executability_rate': x['executable_code_cells'].sum() / x['total_code_cells'].sum() if x['total_code_cells'].sum() > 0 else 0\n",
    "        }, include_groups=False\n",
    "    ).apply(pd.Series)\n",
    "    \n",
    "    return executability\n",
    "\n",
    "# Analyze error types across all agent types\n",
    "def analyze_error_types(df):\n",
    "    \"\"\"\n",
    "    Analyze the types of errors across the dataset.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the processed observations\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with error type counts\n",
    "    \"\"\"\n",
    "    # Process all observations to extract error metadata\n",
    "    processed_df = df.apply(process_observations, axis=1)\n",
    "    \n",
    "    # For coder and reasoning_coder agents\n",
    "    standard_agents = processed_df[\n",
    "        (processed_df['agent_type'].isin(['coder', 'reasoning_coder'])) & \n",
    "        (processed_df['has_error'] == True)\n",
    "    ]\n",
    "    \n",
    "    error_counts = standard_agents['error_type'].value_counts().reset_index()\n",
    "    error_counts.columns = ['Error Type', 'Count']\n",
    "    \n",
    "    # For react agents, we need to flatten the list of error types\n",
    "    react_agents = processed_df[processed_df['agent_type'].isin(['react', 'reasoning_react', 'reasoning_react_v2']) & (processed_df['has_error'] == True)]\n",
    "    react_error_types = []\n",
    "    \n",
    "    for error_types in react_agents['error_type'].dropna():\n",
    "        react_error_types.extend(error_types)\n",
    "    \n",
    "    if react_error_types:\n",
    "        react_error_counts = pd.Series(react_error_types).value_counts().reset_index()\n",
    "        react_error_counts.columns = ['Error Type', 'Count']\n",
    "        \n",
    "        # Combine the error counts\n",
    "        error_counts = pd.concat([error_counts, react_error_counts]).groupby('Error Type').sum().reset_index()\n",
    "    \n",
    "    return error_counts\n",
    "\n",
    "executability_analysis = analyze_executability_rate(df)\n",
    "display(executability_analysis)\n",
    "\n",
    "for agent in ['react', 'coder', 'reasoning_coder', 'reasoning_react', 'reasoning_react_v2', 'combined agents']:\n",
    "    print(f\"Error analysis for {agent} agents:\")\n",
    "    if agent != 'combined agents':\n",
    "        error_analysis = analyze_error_types(df[df['agent_type'] == agent])\n",
    "        display(error_analysis.sort_values(by='Count', ascending=False))\n",
    "    else:\n",
    "        error_analysis = analyze_error_types(df)\n",
    "        display(error_analysis.sort_values(by='Count', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Metrics\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 Score\n",
    "- True Positive\n",
    "- False Positive\n",
    "- True Negative\n",
    "- False Negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before calculating metrics, validate the data preparation\n",
    "assert set(df['clean_final_answer'].unique()).issubset({'true', 'false', 'not verifiable'}), \"Unexpected values in clean_final_answer\"\n",
    "assert set(df['hypothesis_is_true_coerced'].unique()).issubset({'true', 'false'}), \"Unexpected values in hypothesis_is_true_coerced\"\n",
    "\n",
    "# accruacy\n",
    "df['accuracy'] = df['clean_final_answer'] == df['hypothesis_is_true_coerced']\n",
    "print(\"Accuracy:\")\n",
    "display(df['accuracy'].value_counts())\n",
    "\n",
    "# TPR\n",
    "df['true_positive'] = (df['clean_final_answer'] == 'true') & (df['hypothesis_is_true_coerced'] == 'true')\n",
    "print(\"True Positive:\")\n",
    "display(df['true_positive'].value_counts())\n",
    "\n",
    "# TNR\n",
    "df['true_negative'] = (df['clean_final_answer'] == 'false') & (df['hypothesis_is_true_coerced'] == 'false')\n",
    "print(\"True Negative:\")\n",
    "display(df['true_negative'].value_counts())\n",
    "\n",
    "# FPR\n",
    "df['false_positive'] = (df['clean_final_answer'] == 'true') & (df['hypothesis_is_true_coerced'] == 'false')\n",
    "print(\"False Positive:\")\n",
    "display(df['false_positive'].value_counts())\n",
    "\n",
    "# FNR\n",
    "df['false_negative'] = (df['clean_final_answer'] == 'false') & (df['hypothesis_is_true_coerced'] == 'true')\n",
    "print(\"False Negative:\")\n",
    "display(df['false_negative'].value_counts())\n",
    "\n",
    "# aggregate metrics\n",
    "summarized_metrics = {}\n",
    "number_of_true_positives = df['true_positive'].sum()\n",
    "number_of_false_positives = df['false_positive'].sum()\n",
    "number_of_true_negatives = df['true_negative'].sum()\n",
    "number_of_false_negatives = df['false_negative'].sum()\n",
    "\n",
    "# precision\n",
    "summarized_metrics['precision'] = number_of_true_positives / (number_of_true_positives + number_of_false_positives)\n",
    "print(\"Precision:\", summarized_metrics['precision'])\n",
    "\n",
    "# recall\n",
    "summarized_metrics['recall'] = number_of_true_positives / (number_of_true_positives + number_of_false_negatives)\n",
    "print(\"Recall:\", summarized_metrics['recall'])\n",
    "\n",
    "# f1 score\n",
    "summarized_metrics['f1_score'] = 2 * (summarized_metrics['precision'] * summarized_metrics['recall']) / (summarized_metrics['precision'] + summarized_metrics['recall'])\n",
    "print(\"F1 Score:\", summarized_metrics['f1_score'])\n",
    "\n",
    "# confusion matrix\n",
    "# create a confusion matrix\n",
    "confusion_matrix = pd.DataFrame(index=['true', 'false'], columns=['true', 'false'])\n",
    "confusion_matrix.loc['true', 'true'] = number_of_true_positives\n",
    "confusion_matrix.loc['true', 'false'] = number_of_false_positives\n",
    "confusion_matrix.loc['false', 'true'] = number_of_false_negatives\n",
    "confusion_matrix.loc['false', 'false'] = number_of_true_negatives\n",
    "print(\"Confusion Matrix:\")\n",
    "display(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Sanity Check on our Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate confusion matrix components add up correctly\n",
    "total_samples = len(df)\n",
    "total_classified = number_of_true_positives + number_of_false_positives + number_of_true_negatives + number_of_false_negatives\n",
    "not_verifiable_count = total_samples - total_classified\n",
    "assert total_classified + not_verifiable_count == total_samples, \"Confusion matrix components don't add up to total samples\"\n",
    "\n",
    "# Validate that confusion matrix components match their definitions\n",
    "assert number_of_true_positives == df['true_positive'].sum(), \"TP count mismatch\"\n",
    "assert number_of_false_positives == df['false_positive'].sum(), \"FP count mismatch\"\n",
    "assert number_of_true_negatives == df['true_negative'].sum(), \"TN count mismatch\"\n",
    "assert number_of_false_negatives == df['false_negative'].sum(), \"FN count mismatch\"\n",
    "\n",
    "# Validate that no sample is counted in multiple categories\n",
    "assert (df['true_positive'] & df['false_positive']).sum() == 0, \"Sample counted as both TP and FP\"\n",
    "assert (df['true_positive'] & df['true_negative']).sum() == 0, \"Sample counted as both TP and TN\"\n",
    "assert (df['true_positive'] & df['false_negative']).sum() == 0, \"Sample counted as both TP and FN\"\n",
    "assert (df['false_positive'] & df['true_negative']).sum() == 0, \"Sample counted as both FP and TN\"\n",
    "assert (df['false_positive'] & df['false_negative']).sum() == 0, \"Sample counted as both FP and FN\"\n",
    "assert (df['true_negative'] & df['false_negative']).sum() == 0, \"Sample counted as both TN and FN\"\n",
    "\n",
    "# Validate metric calculations\n",
    "# Avoid division by zero\n",
    "if (number_of_true_positives + number_of_false_positives) > 0:\n",
    "    calculated_precision = number_of_true_positives / (number_of_true_positives + number_of_false_positives)\n",
    "    assert abs(calculated_precision - summarized_metrics['precision']) < 1e-10, \"Precision calculation error\"\n",
    "else:\n",
    "    assert 'precision' not in summarized_metrics or pd.isna(summarized_metrics['precision']), \"Precision should be undefined when denominator is zero\"\n",
    "\n",
    "if (number_of_true_positives + number_of_false_negatives) > 0:\n",
    "    calculated_recall = number_of_true_positives / (number_of_true_positives + number_of_false_negatives)\n",
    "    assert abs(calculated_recall - summarized_metrics['recall']) < 1e-10, \"Recall calculation error\"\n",
    "else:\n",
    "    assert 'recall' not in summarized_metrics or pd.isna(summarized_metrics['recall']), \"Recall should be undefined when denominator is zero\"\n",
    "\n",
    "# F1 score validation\n",
    "if 'precision' in summarized_metrics and 'recall' in summarized_metrics and summarized_metrics['precision'] > 0 and summarized_metrics['recall'] > 0:\n",
    "    calculated_f1 = 2 * (summarized_metrics['precision'] * summarized_metrics['recall']) / (summarized_metrics['precision'] + summarized_metrics['recall'])\n",
    "    assert abs(calculated_f1 - summarized_metrics['f1_score']) < 1e-10, \"F1 score calculation error\"\n",
    "else:\n",
    "    assert 'f1_score' not in summarized_metrics or pd.isna(summarized_metrics['f1_score']), \"F1 score should be undefined when precision or recall is zero\"\n",
    "\n",
    "# Validate confusion matrix construction\n",
    "assert confusion_matrix.loc['true', 'true'] == number_of_true_positives, \"TP mismatch in confusion matrix\"\n",
    "assert confusion_matrix.loc['true', 'false'] == number_of_false_positives, \"FP mismatch in confusion matrix\"\n",
    "assert confusion_matrix.loc['false', 'true'] == number_of_false_negatives, \"FN mismatch in confusion matrix\"\n",
    "assert confusion_matrix.loc['false', 'false'] == number_of_true_negatives, \"TN mismatch in confusion matrix\"\n",
    "\n",
    "# Print validation success message\n",
    "print(\"All metric calculations validated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratify Metrics by Agent Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# compute metrics per agent type\n",
    "grouped_by_agent = df.groupby('agent_summary')\n",
    "\n",
    "metrics = {}\n",
    "for agent, group in grouped_by_agent:\n",
    "    metrics[agent] = {}\n",
    "    \n",
    "    # sum for TPR, TNR, FPR, FNR\n",
    "    number_of_true_positives_group = group['true_positive'].sum()\n",
    "    number_of_false_positives_group = group['false_positive'].sum()\n",
    "    number_of_true_negatives_group = group['true_negative'].sum()\n",
    "    number_of_false_negatives_group = group['false_negative'].sum()\n",
    "    metrics[agent]['true_positive_sum'] = number_of_true_positives_group\n",
    "    metrics[agent]['true_negative_sum'] = number_of_true_negatives_group\n",
    "    metrics[agent]['false_positive_sum'] = number_of_false_positives_group\n",
    "    metrics[agent]['false_negative_sum'] = number_of_false_negatives_group\n",
    "\n",
    "    # mean TPR, TNR, FPR, FNR\n",
    "    metrics[agent]['true_positive_mean'] = group['true_positive'].mean()\n",
    "    metrics[agent]['true_negative_mean'] = group['true_negative'].mean()\n",
    "    metrics[agent]['false_positive_mean'] = group['false_positive'].mean()\n",
    "    metrics[agent]['false_negative_mean'] = group['false_negative'].mean()\n",
    "\n",
    "    # precision\n",
    "    if (number_of_true_positives_group + number_of_false_positives_group) > 0:\n",
    "        group_precision = number_of_true_positives_group / (number_of_true_positives_group + number_of_false_positives_group)\n",
    "    else:\n",
    "        group_precision = float('nan')\n",
    "    metrics[agent]['precision'] = group_precision\n",
    "    \n",
    "    # recall\n",
    "    if (number_of_true_positives_group + number_of_false_negatives_group) > 0:\n",
    "        group_recall = number_of_true_positives_group / (number_of_true_positives_group + number_of_false_negatives_group)\n",
    "    else:\n",
    "        group_recall = float('nan')\n",
    "    metrics[agent]['recall'] = group_recall\n",
    "    \n",
    "    # f1 score\n",
    "    if group_precision > 0 and group_recall > 0:\n",
    "        metrics[agent]['f1_score'] = 2 * (group_precision * group_recall) / (group_precision + group_recall)\n",
    "    else:\n",
    "        metrics[agent]['f1_score'] = float('nan')\n",
    "\n",
    "# Convert metrics to DataFrame for easier visualization\n",
    "metrics_df = pd.DataFrame.from_dict(metrics, orient='index')\n",
    "\n",
    "# Create visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Bar plots for key metrics\n",
    "plt.figure(figsize=(15, 10))\n",
    "metrics_to_plot = ['precision', 'recall', 'f1_score', \n",
    "                   'true_positive_mean', 'true_negative_mean', \n",
    "                   'false_positive_mean', 'false_negative_mean']\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    sns.barplot(x=metrics_df.index, y=metrics_df[metric])\n",
    "    plt.title(f'{metric}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)  # All these metrics are between 0 and 1\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Confusion matrices for each agent\n",
    "for agent in metrics:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = np.array([\n",
    "        [metrics[agent]['true_positive_sum'], metrics[agent]['false_negative_sum']],\n",
    "        [metrics[agent]['false_positive_sum'], metrics[agent]['true_negative_sum']]\n",
    "    ])\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Predicted True', 'Predicted False'],\n",
    "                yticklabels=['Actually True', 'Actually False'])\n",
    "    \n",
    "    plt.title(f'Confusion Matrix - {agent}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 3. Combined metrics comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics_df[['precision', 'recall', 'f1_score']].plot(kind='bar')\n",
    "plt.title('Performance Metrics by Agent Type')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Radar chart for comprehensive comparison\n",
    "def radar_plot(metrics_df):\n",
    "    # Select metrics for radar plot\n",
    "    radar_metrics = ['precision', 'recall', 'f1_score', \n",
    "                     'true_positive_mean', 'true_negative_mean']\n",
    "    \n",
    "    # Number of variables\n",
    "    categories = radar_metrics\n",
    "    N = len(categories)\n",
    "    \n",
    "    # Create a figure\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Create angles for each metric\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Close the loop\n",
    "    \n",
    "    # Create subplot in polar projection\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    \n",
    "    # Draw one axis per variable and add labels\n",
    "    plt.xticks(angles[:-1], categories, size=12)\n",
    "    \n",
    "    # Draw ylabels\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([0.25, 0.5, 0.75], [\"0.25\", \"0.5\", \"0.75\"], size=10)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Plot each agent\n",
    "    for agent in metrics_df.index:\n",
    "        values = metrics_df.loc[agent, radar_metrics].values.flatten().tolist()\n",
    "        values += values[:1]  # Close the loop\n",
    "        \n",
    "        # Plot values\n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=agent)\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title('Agent Performance Comparison', size=15)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "radar_fig = radar_plot(metrics_df)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Return the metrics DataFrame for further analysis\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(metrics_df.sort_values(by='false_positive_mean', ascending=False))\n",
    "display(metrics_df.sort_values(by='false_negative_mean', ascending=False))\n",
    "# display(metrics_df.sort_values(by='precision', ascending=False))\n",
    "# display(metrics_df.sort_values(by='recall', ascending=False))\n",
    "# display(metrics_df.sort_values(by='f1_score', ascending=False))\n",
    "# display(metrics_df.sort_values(by='true_positive_mean', ascending=False))\n",
    "# display(metrics_df.sort_values(by='true_negative_mean', ascending=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show counts by agent type\n",
    "df['final_answer'].value_counts()\n",
    "\n",
    "\n",
    "# Extract 5 random samples from each agent type\n",
    "agent_types = df['agent_type'].unique()\n",
    "samples = pd.DataFrame()\n",
    "\n",
    "for agent in ['reasoning_react']:\n",
    "    agent_samples = []\n",
    "    # agent_samples.append(df[(df['agent_type'] == agent) & (df['final_answer'] == 'True')].sample(2))\n",
    "    # agent_samples.append(df[(df['agent_type'] == agent) & (df['final_answer'] == 'False')].sample(2))\n",
    "    agent_samples.append(df[(df['agent_type'] == agent) & (df['final_answer'] == 'Not Verifiable')].sample(1))\n",
    "    samples = pd.concat([samples, *agent_samples])\n",
    "\n",
    "# Display the samples\n",
    "# display(samples.head(2))\n",
    "\n",
    "for idx, sample in samples.iterrows():\n",
    "    \n",
    "    agent_type = sample['agent_type']\n",
    "    \n",
    "        \n",
    "    print(f\"\\nSample {idx}:\")\n",
    "    print(\"hypothesis:\", sample['hypothesis'])\n",
    "    print(\"final_answer:\", sample['final_answer'])\n",
    "    print(\"hypothesis_is_true:\", sample['hypothesis_is_true'])\n",
    "    print(\"agent_type:\", sample['agent_type'])\n",
    "    print(\"model_name:\", sample['model_name'])\n",
    "    print(\"planning_model:\", sample['planning_model'])\n",
    "    print(\"coding_model:\", sample['coding_model'])\n",
    "    print(\"step_count:\", sample['step_count'])\n",
    "    \n",
    "    if agent_type == 'react':\n",
    "        observations = sample['observations']\n",
    "        print(observations)\n",
    "        print(sample['code'])  \n",
    "            \n",
    "    elif agent_type == \"coder\":\n",
    "        print(\"code:\\n\", sample['code'])\n",
    "        print(\"observations:\\n\", sample['observations'])\n",
    "    \n",
    "    elif agent_type == \"reasoning_coder\":\n",
    "        print(\"analysis_plan:\\n\", sample['analysis_plan'])\n",
    "        print(\"code:\\n\", sample['code'])\n",
    "        print(\"observations:\\n\", sample['observations'])\n",
    "    \n",
    "    print(\"final_answer:\", sample['final_answer'])\n",
    "    print(\"hypothesis_is_true:\", sample['hypothesis_is_true'])\n",
    "    \n",
    "    print(\"evidence:\")\n",
    "    for evidence in sample['evidence']:\n",
    "        print(\"-\", evidence)\n",
    "        \n",
    "    print(\"\\npmid:\", sample['pmid'])\n",
    "    print(\"hypothesis_index:\", sample['hypothesis_index'])\n",
    "    print(\"dataset_ids:\", sample['dataset_ids'])\n",
    "    \n",
    "    print(\"------------------\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evidence Alignement Analysis\n",
    "\n",
    "### Data Ingestion\n",
    "- load the evidence alignment results\n",
    "- unpack the experiment config results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from typing import List, Dict\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('/home/ubuntu/BioDSA') # this will need to be updated for other machines/folders\n",
    "from src import (\n",
    "    REPO_ROOT,\n",
    "    TOP_LEVEL_LOG_DIR,\n",
    "    HYPOTHESIS_DIR\n",
    ")\n",
    "\n",
    "EVIDENCE_ALIGNMENT_RESULTS_FILE = os.path.join( os.path.join(TOP_LEVEL_LOG_DIR, \"eval_evidence_alignment\"), \"eval_results.json\")\n",
    "\n",
    "alignment_df = pd.read_json(EVIDENCE_ALIGNMENT_RESULTS_FILE)\n",
    "\n",
    "# unpack the experiment_config column into separate columns\n",
    "\n",
    "alignment_df = pd.concat([alignment_df.drop('experiment_config', axis=1), alignment_df['experiment_config'].apply(pd.Series)], axis=1)\n",
    "\n",
    "alignment_df['agent_summary'] = alignment_df.apply(get_agent_type, axis=1)\n",
    "# display(df['agent_summary'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Summarized Alignment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a summary of the alignment results\n",
    "def summarize_alignment_results(row: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Summarize the alignment results for a given row.\n",
    "    \"\"\"\n",
    "    ground_truth_evidence = row['ground_truth_evidence']\n",
    "    generated_evidence = row['generated_evidence']\n",
    "    alignment_results = row['eval_evidence_alignment']\n",
    "    \n",
    "    if not len(ground_truth_evidence) == len(alignment_results):\n",
    "        print(f\"WARNING: Length mismatch between ground_truth_evidence and alignment_results for row {row}\")\n",
    "        # print(\"ground_truth_evidence: \", ground_truth_evidence)\n",
    "        # print(\"alignment_results: \", alignment_results)\n",
    "        # print(\"generated_evidence: \", generated_evidence)\n",
    "        # raise ValueError(\"Length mismatch between ground_truth_evidence and alignment_results\")\n",
    "    \n",
    "    values = {\n",
    "        \"supported\": 0,\n",
    "        \"contradicted\": 0,\n",
    "        \"missed\": 0\n",
    "    }\n",
    "    for i in range(len(ground_truth_evidence)):\n",
    "        res = alignment_results[i]['alignment']\n",
    "        res = res.strip().lower()\n",
    "        \n",
    "        if res not in ['supported', 'contradicted', 'missed']:\n",
    "            raise ValueError(f\"Invalid alignment result: {res}\")\n",
    "        \n",
    "        values[res] += 1\n",
    "        \n",
    "    for key, value in values.items():\n",
    "        row[f\"alignment_eval_{key}\"] = value\n",
    "        \n",
    "    return row\n",
    "\n",
    "alignment_df = alignment_df.apply(summarize_alignment_results, axis=1)\n",
    "alignment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-verifiable Hypothesis Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nv_logs_directory_path = os.path.join(TOP_LEVEL_LOG_DIR, 'experiment_logs_non-verifiable')\n",
    "nv_results = get_experiment_results(nv_logs_directory_path)\n",
    "\n",
    "# create a dataframe from the results\n",
    "df_nv = pd.DataFrame(nv_results)\n",
    "\n",
    "# unpack the experiment_config column into separate columns\n",
    "df_nv = pd.concat([df_nv.drop('experiment_config', axis=1), df_nv['experiment_config'].apply(pd.Series)], axis=1)\n",
    "\n",
    "# extract the agent type, which is composed of the agent_type, and it's hyperparameters\n",
    "def get_agent_type(row: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Format agent type based on configuration.\n",
    "    For react agent: (react, step_count)\n",
    "    For reasoning coder: (reasoning_coder, planning_model, coding_model)\n",
    "    For coder: (coder, model_name)\n",
    "    \"\"\"\n",
    "    agent_type = row[\"agent_type\"]\n",
    "    if agent_type == \"react\":\n",
    "        return f\"(react, {row['step_count']}, {row['model_name']})\"\n",
    "    elif agent_type == \"reasoning_coder\":\n",
    "        return f\"(reasoning_coder, {row['planning_model']}, {row['coding_model']})\"\n",
    "    elif agent_type == \"coder\":\n",
    "        return f\"(coder, {row['model_name']})\"\n",
    "    elif agent_type == \"reasoning_react\":\n",
    "        return f\"(reasoning_react, {row['plan_model_name']}, {row['agent_model_name']}, {row['step_count']})\"\n",
    "    elif agent_type == \"reasoning_react_v2\":\n",
    "        return f\"(reasoning_react_v2, {row['plan_model_name']}, {row['agent_model_name']}, {row['step_count']})\"\n",
    "    return agent_type\n",
    "\n",
    "df_nv['agent_summary'] = df_nv.apply(get_agent_type, axis=1)\n",
    "display(df_nv['agent_summary'].value_counts())\n",
    "\n",
    "# sanity check in case some of the llms outputted weird values\n",
    "display(df_nv['final_answer'].value_counts()) # generated by the agents\n",
    "display(df_nv['hypothesis_is_true'].value_counts()) # ground truth\n",
    "\n",
    "# Standardize the final answer values\n",
    "df_nv['coerced_final_answer'] = df_nv['final_answer'].astype(str).str.lower()\n",
    "df_nv['hypothesis_is_true_coerced'] = df_nv['hypothesis_is_true'].astype(str).str.lower()\n",
    "\n",
    "# Check unique values after standardization\n",
    "print(\"Unique values in coerced_final_answer before standardization:\")\n",
    "display(df_nv['coerced_final_answer'].value_counts())\n",
    "print(\"Unique values in hypothesis_is_true_coerced before standardization:\")\n",
    "display(df_nv['hypothesis_is_true_coerced'].value_counts())\n",
    "\n",
    "def standardize_agent_response(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Standardize the agent response to a consistent format.\n",
    "    \"\"\"\n",
    "    if 'true' in response:\n",
    "        return 'true'\n",
    "    elif 'false' in response:\n",
    "        return 'false'\n",
    "    else:\n",
    "        return 'not verifiable'\n",
    "\n",
    "# Create a cleaner version of final answer that handles variations\n",
    "df_nv['clean_final_answer'] = df_nv['coerced_final_answer'].apply(\n",
    "    standardize_agent_response\n",
    ")\n",
    "\n",
    "print(\"Unique values in clean_final_answer after standardization:\")\n",
    "display(df_nv['clean_final_answer'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executability_analysis_nv = analyze_executability_rate(df_nv)\n",
    "display(executability_analysis_nv)\n",
    "\n",
    "for agent in ['react', 'coder', 'reasoning_coder', 'combined agents']:\n",
    "    print(f\"Error analysis for {agent} agents:\")\n",
    "    if agent != 'combined agents':\n",
    "        error_analysis = analyze_error_types(df_nv[df_nv['agent_type'] == agent])\n",
    "        display(error_analysis.sort_values(by='Count', ascending=False))\n",
    "    else:\n",
    "        error_analysis = analyze_error_types(df_nv)\n",
    "        display(error_analysis.sort_values(by='Count', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before calculating metrics, validate the data preparation\n",
    "assert set(df_nv['clean_final_answer'].unique()).issubset({'true', 'false', 'not verifiable'}), \"Unexpected values in clean_final_answer\"\n",
    "assert set(df_nv['hypothesis_is_true_coerced'].unique()).issubset({'true', 'false'}), \"Unexpected values in hypothesis_is_true_coerced\"\n",
    "\n",
    "# accruacy\n",
    "df_nv['accuracy'] = df_nv['clean_final_answer'] == df_nv['hypothesis_is_true_coerced']\n",
    "print(\"Accuracy:\")\n",
    "display(df_nv['accuracy'].value_counts())\n",
    "\n",
    "# TPR\n",
    "df_nv['true_positive'] = (df_nv['clean_final_answer'] == 'true') & (df_nv['hypothesis_is_true_coerced'] == 'true')\n",
    "print(\"True Positive:\")\n",
    "display(df_nv['true_positive'].value_counts())\n",
    "\n",
    "# TNR\n",
    "df_nv['true_negative'] = (df_nv['clean_final_answer'] == 'false') & (df_nv['hypothesis_is_true_coerced'] == 'false')\n",
    "print(\"True Negative:\")\n",
    "display(df_nv['true_negative'].value_counts())\n",
    "\n",
    "# FPR\n",
    "df_nv['false_positive'] = (df_nv['clean_final_answer'] == 'true') & (df_nv['hypothesis_is_true_coerced'] == 'false')\n",
    "print(\"False Positive:\")\n",
    "display(df_nv['false_positive'].value_counts())\n",
    "\n",
    "# FNR\n",
    "df_nv['false_negative'] = (df_nv['clean_final_answer'] == 'false') & (df_nv['hypothesis_is_true_coerced'] == 'true')\n",
    "print(\"False Negative:\")\n",
    "display(df_nv['false_negative'].value_counts())\n",
    "\n",
    "# aggregate metrics\n",
    "summarized_metrics_nv = {}\n",
    "number_of_true_positives_nv = df_nv['true_positive'].sum()\n",
    "number_of_false_positives_nv = df_nv['false_positive'].sum()\n",
    "number_of_true_negatives_nv = df_nv['true_negative'].sum()\n",
    "number_of_false_negatives_nv = df_nv['false_negative'].sum()\n",
    "\n",
    "# precision\n",
    "summarized_metrics_nv['precision'] = number_of_true_positives_nv / (number_of_true_positives_nv + number_of_false_positives_nv)\n",
    "print(\"Precision:\", summarized_metrics_nv['precision'])\n",
    "\n",
    "# recall\n",
    "summarized_metrics_nv['recall'] = number_of_true_positives_nv / (number_of_true_positives_nv + number_of_false_negatives_nv)\n",
    "print(\"Recall:\", summarized_metrics_nv['recall'])\n",
    "\n",
    "# f1 score\n",
    "summarized_metrics_nv['f1_score'] = 2 * (summarized_metrics_nv['precision'] * summarized_metrics_nv['recall']) / (summarized_metrics_nv['precision'] + summarized_metrics_nv['recall'])\n",
    "print(\"F1 Score:\", summarized_metrics_nv['f1_score'])\n",
    "\n",
    "# confusion matrix\n",
    "# create a confusion matrix\n",
    "confusion_matrix_nv = pd.DataFrame(index=['true', 'false'], columns=['true', 'false'])\n",
    "confusion_matrix_nv.loc['true', 'true'] = number_of_true_positives_nv\n",
    "confusion_matrix_nv.loc['true', 'false'] = number_of_false_positives_nv\n",
    "confusion_matrix_nv.loc['false', 'true'] = number_of_false_negatives_nv\n",
    "confusion_matrix_nv.loc['false', 'false'] = number_of_true_negatives_nv\n",
    "print(\"Confusion Matrix:\")\n",
    "display(confusion_matrix_nv)\n",
    "\n",
    "\n",
    "## Sanity checking\n",
    "# Validate confusion matrix components add up correctly\n",
    "total_samples_nv = len(df_nv)\n",
    "total_classified_nv = number_of_true_positives_nv + number_of_false_positives_nv + number_of_true_negatives_nv + number_of_false_negatives_nv\n",
    "not_verifiable_count_nv = total_samples_nv - total_classified_nv\n",
    "assert total_classified_nv + not_verifiable_count_nv == total_samples_nv, \"Confusion matrix components don't add up to total samples\"\n",
    "\n",
    "# Validate that confusion matrix components match their definitions\n",
    "assert number_of_true_positives_nv == df_nv['true_positive'].sum(), \"TP count mismatch\"\n",
    "assert number_of_false_positives_nv == df_nv['false_positive'].sum(), \"FP count mismatch\"\n",
    "assert number_of_true_negatives_nv == df_nv['true_negative'].sum(), \"TN count mismatch\"\n",
    "assert number_of_false_negatives_nv == df_nv['false_negative'].sum(), \"FN count mismatch\"\n",
    "\n",
    "# Validate that no sample is counted in multiple categories\n",
    "assert (df_nv['true_positive'] & df_nv['false_positive']).sum() == 0, \"Sample counted as both TP and FP\"\n",
    "assert (df_nv['true_positive'] & df_nv['true_negative']).sum() == 0, \"Sample counted as both TP and TN\"\n",
    "assert (df_nv['true_positive'] & df_nv['false_negative']).sum() == 0, \"Sample counted as both TP and FN\"\n",
    "assert (df_nv['false_positive'] & df_nv['true_negative']).sum() == 0, \"Sample counted as both FP and TN\"\n",
    "assert (df_nv['false_positive'] & df_nv['false_negative']).sum() == 0, \"Sample counted as both FP and FN\"\n",
    "assert (df_nv['true_negative'] & df_nv['false_negative']).sum() == 0, \"Sample counted as both TN and FN\"\n",
    "\n",
    "# Validate metric calculations\n",
    "# Avoid division by zero\n",
    "if (number_of_true_positives_nv + number_of_false_positives_nv) > 0:\n",
    "    calculated_precision_nv = number_of_true_positives_nv / (number_of_true_positives_nv + number_of_false_positives_nv)\n",
    "    assert abs(calculated_precision_nv - summarized_metrics_nv['precision']) < 1e-10, \"Precision calculation error\"\n",
    "else:\n",
    "    assert 'precision' not in summarized_metrics_nv or pd.isna(summarized_metrics_nv['precision']), \"Precision should be undefined when denominator is zero\"\n",
    "\n",
    "if (number_of_true_positives_nv + number_of_false_negatives_nv) > 0:\n",
    "    calculated_recall_nv = number_of_true_positives_nv / (number_of_true_positives_nv + number_of_false_negatives_nv)\n",
    "    assert abs(calculated_recall_nv - summarized_metrics_nv['recall']) < 1e-10, \"Recall calculation error\"\n",
    "else:\n",
    "    assert 'recall' not in summarized_metrics_nv or pd.isna(summarized_metrics_nv['recall']), \"Recall should be undefined when denominator is zero\"\n",
    "\n",
    "# F1 score validation\n",
    "if 'precision' in summarized_metrics_nv and 'recall' in summarized_metrics_nv and summarized_metrics_nv['precision'] > 0 and summarized_metrics_nv['recall'] > 0:\n",
    "    calculated_f1_nv = 2 * (summarized_metrics_nv['precision'] * summarized_metrics_nv['recall']) / (summarized_metrics_nv['precision'] + summarized_metrics_nv['recall'])\n",
    "    assert abs(calculated_f1_nv - summarized_metrics_nv['f1_score']) < 1e-10, \"F1 score calculation error\"\n",
    "else:\n",
    "    assert 'f1_score' not in summarized_metrics_nv or pd.isna(summarized_metrics_nv['f1_score']), \"F1 score should be undefined when precision or recall is zero\"\n",
    "\n",
    "# Validate confusion matrix construction\n",
    "assert confusion_matrix_nv.loc['true', 'true'] == number_of_true_positives_nv, \"TP mismatch in confusion matrix\"\n",
    "assert confusion_matrix_nv.loc['true', 'false'] == number_of_false_positives_nv, \"FP mismatch in confusion matrix\"\n",
    "assert confusion_matrix_nv.loc['false', 'true'] == number_of_false_negatives_nv, \"FN mismatch in confusion matrix\"\n",
    "assert confusion_matrix_nv.loc['false', 'false'] == number_of_true_negatives_nv, \"TN mismatch in confusion matrix\"\n",
    "\n",
    "# Print validation success message\n",
    "print(\"All metric calculations validated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics per agent type\n",
    "grouped_by_agent_nv = df_nv.groupby('agent_summary')\n",
    "\n",
    "metrics_nv = {}\n",
    "for agent, group in grouped_by_agent_nv:\n",
    "    metrics_nv[agent] = {}\n",
    "    \n",
    "    # sum for TPR, TNR, FPR, FNR\n",
    "    number_of_true_positives_group = group['true_positive'].sum()\n",
    "    number_of_false_positives_group = group['false_positive'].sum()\n",
    "    number_of_true_negatives_group = group['true_negative'].sum()\n",
    "    number_of_false_negatives_group = group['false_negative'].sum()\n",
    "    metrics_nv[agent]['true_positive_sum'] = number_of_true_positives_group\n",
    "    metrics_nv[agent]['true_negative_sum'] = number_of_true_negatives_group\n",
    "    metrics_nv[agent]['false_positive_sum'] = number_of_false_positives_group\n",
    "    metrics_nv[agent]['false_negative_sum'] = number_of_false_negatives_group\n",
    "\n",
    "    # mean TPR, TNR, FPR, FNR\n",
    "    metrics_nv[agent]['true_positive_mean'] = group['true_positive'].mean()\n",
    "    metrics_nv[agent]['true_negative_mean'] = group['true_negative'].mean()\n",
    "    metrics_nv[agent]['false_positive_mean'] = group['false_positive'].mean()\n",
    "    metrics_nv[agent]['false_negative_mean'] = group['false_negative'].mean()\n",
    "\n",
    "    # precision\n",
    "    if (number_of_true_positives_group + number_of_false_positives_group) > 0:\n",
    "        group_precision_nv = number_of_true_positives_group / (number_of_true_positives_group + number_of_false_positives_group)\n",
    "    else:\n",
    "        group_precision_nv = float('nan')\n",
    "    metrics_nv[agent]['precision'] = group_precision_nv\n",
    "    \n",
    "    # recall\n",
    "    if (number_of_true_positives_group + number_of_false_negatives_group) > 0:\n",
    "        group_recall_nv = number_of_true_positives_group / (number_of_true_positives_group + number_of_false_negatives_group)\n",
    "    else:\n",
    "        group_recall_nv = float('nan')\n",
    "    metrics_nv[agent]['recall'] = group_recall_nv\n",
    "    \n",
    "    # f1 score\n",
    "    if group_precision_nv > 0 and group_recall_nv > 0:\n",
    "        metrics_nv[agent]['f1_score'] = 2 * (group_precision_nv * group_recall_nv) / (group_precision_nv + group_recall_nv)\n",
    "    else:\n",
    "        metrics_nv[agent]['f1_score'] = float('nan')\n",
    "\n",
    "# Convert metrics to DataFrame for easier visualization\n",
    "metrics_df_nv = pd.DataFrame.from_dict(metrics_nv, orient='index')\n",
    "\n",
    "# Create visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Bar plots for key metrics\n",
    "plt.figure(figsize=(15, 10))\n",
    "metrics_to_plot = ['precision', 'recall', 'f1_score', \n",
    "                   'true_positive_mean', 'true_negative_mean', \n",
    "                   'false_positive_mean', 'false_negative_mean']\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    sns.barplot(x=metrics_df.index, y=metrics_df[metric])\n",
    "    plt.title(f'{metric}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)  # All these metrics are between 0 and 1\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Confusion matrices for each agent\n",
    "for agent in metrics_nv:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = np.array([\n",
    "        [metrics_nv[agent]['true_positive_sum'], metrics_nv[agent]['false_negative_sum']],\n",
    "        [metrics_nv[agent]['false_positive_sum'], metrics_nv[agent]['true_negative_sum']]\n",
    "    ])\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Predicted True', 'Predicted False'],\n",
    "                yticklabels=['Actually True', 'Actually False'])\n",
    "    \n",
    "    plt.title(f'Confusion Matrix - {agent}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 3. Combined metrics comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics_df_nv[['precision', 'recall', 'f1_score']].plot(kind='bar')\n",
    "plt.title('Performance Metrics by Agent Type')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Radar chart for comprehensive comparison\n",
    "def radar_plot(metrics_df):\n",
    "    # Select metrics for radar plot\n",
    "    radar_metrics = ['precision', 'recall', 'f1_score', \n",
    "                     'true_positive_mean', 'true_negative_mean']\n",
    "    \n",
    "    # Number of variables\n",
    "    categories = radar_metrics\n",
    "    N = len(categories)\n",
    "    \n",
    "    # Create a figure\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Create angles for each metric\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Close the loop\n",
    "    \n",
    "    # Create subplot in polar projection\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    \n",
    "    # Draw one axis per variable and add labels\n",
    "    plt.xticks(angles[:-1], categories, size=12)\n",
    "    \n",
    "    # Draw ylabels\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([0.25, 0.5, 0.75], [\"0.25\", \"0.5\", \"0.75\"], size=10)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Plot each agent\n",
    "    for agent in metrics_df.index:\n",
    "        values = metrics_df.loc[agent, radar_metrics].values.flatten().tolist()\n",
    "        values += values[:1]  # Close the loop\n",
    "        \n",
    "        # Plot values\n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=agent)\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title('Agent Performance Comparison', size=15)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "radar_fig = radar_plot(metrics_df_nv)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Return the metrics DataFrame for further analysis\n",
    "metrics_df_nv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioDSA-T-z_U1Ew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
